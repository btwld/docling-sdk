import { Docling } from "../src";

async function s3SourceExample() {
  console.log("‚òÅÔ∏è S3 Source Integration Example");

  if (!process.env.AWS_ACCESS_KEY_ID || !process.env.AWS_SECRET_ACCESS_KEY) {
    console.log("‚ö†Ô∏è AWS credentials not found in environment variables");
    console.log(
      "   Set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY to test S3 features"
    );
    console.log("   Or configure AWS credentials via AWS CLI or IAM roles");
    return;
  }

  const client = new Docling({
    api: {
      baseUrl: process.env.DOCLING_URL || "http://localhost:5001",
      apiKey: process.env.DOCLING_API_KEY,
      timeout: 120000, // Longer timeout for S3 operations
    },
  });

  try {
    const s3Config = {
      bucket: process.env.S3_BUCKET!,
      key: process.env.S3_KEY!,
      region: process.env.AWS_REGION!,
    };

    // Convert document directly from S3
    const result = await client.convertFromS3(s3Config, {
      to_formats: ["md", "json"],
      do_table_structure: true,
      do_picture_description: true,
    });

    if (result.success) {
      if ("document" in result.data) {
        // ConvertDocumentResponse - has document content
        console.log("‚úÖ S3 document conversion successful!");
        console.log(`üìä Document filename: ${result.data.document.filename}`);
        console.log(`üìÑ Content available: ${result.data.document.json_content ? 'Yes' : 'No'}`);

        if (result.data.document.md_content) {
          const mdLength = result.data.document.md_content.length;
          console.log(`üìù Markdown content: ${mdLength} characters`);
          console.log(`üìù Preview: ${result.data.document.md_content.slice(0, 200)}...`);
        }

        if (result.data.document.json_content) {
          console.log("üîç JSON structure available for further processing");
        }
      } else {
        // PresignedUrlConvertDocumentResponse - only has processing stats
        console.log("‚úÖ S3 conversion completed!");
        console.log(`üìÑ Documents processed: ${result.data.num_converted}`);
        console.log(`üìÑ Documents succeeded: ${result.data.num_succeeded}`);
        console.log(`üìÑ Documents failed: ${result.data.num_failed}`);
      }
    } else {
      console.error("‚ùå S3 conversion failed:", result.error?.message);
    }
  } catch (error) {
    console.error(
      "üí• S3 source error:",
      error instanceof Error ? error.message : error
    );
  }
}

async function s3TargetExample() {
  console.log("\n‚òÅÔ∏è S3 Target Upload Example");

  if (!process.env.AWS_ACCESS_KEY_ID || !process.env.AWS_SECRET_ACCESS_KEY) {
    console.log("‚ö†Ô∏è AWS credentials not found, skipping S3 target example");
    return;
  }

  const client = new Docling({
    api: {
      baseUrl: process.env.DOCLING_URL || "http://localhost:5001",
      apiKey: process.env.DOCLING_API_KEY,
      timeout: 120000,
    },
  });

  try {
    // Create a test document
    const testDocument = Buffer.from(`
# S3 Target Upload Test

This document will be processed and uploaded directly to S3.

## Features Demonstrated
- Document processing
- Direct S3 upload
- Multiple format output
- AWS integration

## Sample Content

### Performance Metrics
| Metric | Value | Target |
|--------|-------|--------|
| Throughput | 95% | 90% |
| Accuracy | 99.2% | 98% |
| Speed | 1.2s | 1.5s |

### Processing Steps
1. Document ingestion
2. Content analysis
3. Format conversion
4. S3 upload
5. Verification

## Conclusion
Direct S3 integration provides seamless cloud workflow integration.
    `);

    const s3Target = {
      bucket: process.env.S3_OUTPUT_BUCKET || "my-docling-output-bucket",
      key:
        process.env.S3_OUTPUT_KEY || `test-outputs/processed-${Date.now()}.zip`,
      region: process.env.AWS_REGION || "us-east-1",
    };

    console.log(
      `üì§ Processing and uploading to S3: s3://${s3Target.bucket}/${s3Target.key}`
    );

    // Process document and upload directly to S3
    const result = await client.convertWithTarget(
      [
        {
          kind: "file",
          base64_string: testDocument.toString("base64"),
          filename: "s3-test.md",
        },
      ],
      {
        kind: "s3",
        bucket: s3Target.bucket,
        key: s3Target.key,
        region: s3Target.region,
      },
      {
        to_formats: ["md", "json", "html"],
        do_table_structure: true,
      }
    );

    if (result.success) {
      console.log("‚úÖ S3 target upload successful!");
      console.log(
        `üéØ Results uploaded to: s3://${s3Target.bucket}/${s3Target.key}`
      );
      console.log(`üìä Processing completed successfully`);
      console.log(`üéØ Results uploaded to S3 target`);
    } else {
      console.error("‚ùå S3 target upload failed:", result.error?.message);
    }
  } catch (error) {
    console.error(
      "üí• S3 target error:",
      error instanceof Error ? error.message : error
    );
  }
}

async function s3BatchProcessing() {
  console.log("\nüì¶ S3 Batch Processing Example");

  if (!process.env.AWS_ACCESS_KEY_ID || !process.env.AWS_SECRET_ACCESS_KEY) {
    console.log("‚ö†Ô∏è AWS credentials not found, skipping S3 batch example");
    return;
  }

  const client = new Docling({
    api: {
      baseUrl: process.env.DOCLING_URL || "http://localhost:5001",
      apiKey: process.env.DOCLING_API_KEY,
      timeout: 180000, // Extended timeout for batch processing
    },
  });

  try {
    // Define multiple S3 sources for batch processing
    const s3Sources = [
      {
        bucket: process.env.S3_BUCKET || "my-docling-input-bucket",
        key: "documents/sample-document-1.pdf",
        region: process.env.AWS_REGION || "us-east-1",
      },
      {
        bucket: process.env.S3_BUCKET || "my-docling-input-bucket",
        key: "documents/sample-document-2.pdf",
        region: process.env.AWS_REGION || "us-east-1",
      },
      {
        bucket: process.env.S3_BUCKET || "my-docling-input-bucket",
        key: "documents/sample-document-3.pdf",
        region: process.env.AWS_REGION || "us-east-1",
      },
    ];

    const outputBucket = process.env.S3_OUTPUT_BUCKET || "my-docling-output-bucket";
    const outputRegion = process.env.AWS_REGION || "us-east-1";

    console.log(`üìö Processing ${s3Sources.length} documents from S3...`);

    // Process documents in parallel with individual S3 targets
    const results = await Promise.all(
      s3Sources.map(async (source, index) => {
        try {
          console.log(
            `üîÑ Processing document ${index + 1}: s3://${source.bucket}/${
              source.key
            }`
          );

          const outputKey = `test-outputs/batch-results/processed-${
            index + 1
          }-${Date.now()}.zip`;

          const result = await client.convertWithTarget(
            [{ kind: "s3", ...source }],
            {
              kind: "s3",
              bucket: outputBucket,
              key: outputKey,
              region: outputRegion,
            },
            {
              to_formats: ["md", "json"],
              do_table_structure: true,
            }
          );

          return {
            index: index + 1,
            source: `s3://${source.bucket}/${source.key}`,
            target: `s3://${outputBucket}/${outputKey}`,
            success: result.success,
            error: result.success ? null : result.error?.message,
          };
        } catch (error) {
          return {
            index: index + 1,
            source: `s3://${source.bucket}/${source.key}`,
            target: "N/A",
            success: false,
            error: error instanceof Error ? error.message : "Unknown error",
          };
        }
      })
    );

    // Display batch results
    console.log("\nüìä S3 Batch Processing Results:");
    results.forEach(({ index, source, target, success, error }) => {
      if (success) {
        console.log(`  ‚úÖ Document ${index}: ${source} ‚Üí ${target}`);
      } else {
        console.log(`  ‚ùå Document ${index}: ${source} - ${error}`);
      }
    });

    const successCount = results.filter((r) => r.success).length;
    console.log(
      `\nüéØ Batch completed: ${successCount}/${results.length} successful`
    );
  } catch (error) {
    console.error(
      "üí• S3 batch processing error:",
      error instanceof Error ? error.message : error
    );
  }
}

async function s3WithProgress() {
  console.log("\nüìä S3 Processing with Progress Tracking");

  if (!process.env.AWS_ACCESS_KEY_ID || !process.env.AWS_SECRET_ACCESS_KEY) {
    console.log("‚ö†Ô∏è AWS credentials not found, skipping S3 progress example");
    return;
  }

  const client = new Docling({
    api: {
      baseUrl: process.env.DOCLING_URL || "http://localhost:5001",
      apiKey: process.env.DOCLING_API_KEY,
      timeout: 120000,
    },
    progress: {
      method: "websocket",
      onProgress: (progress) => {
        const percentage = progress.percentage ?? 0;
        const stage = progress.stage || "processing";
        const progressBar =
          "‚ñà".repeat(Math.floor(percentage / 5)) +
          "‚ñë".repeat(20 - Math.floor(percentage / 5));

        process.stdout.write(
          `\r‚òÅÔ∏è S3 ${stage}: [${progressBar}] ${percentage.toFixed(1)}%`
        );

        if (progress.uploadedBytes && progress.totalBytes) {
          const speed = progress.bytesPerSecond
            ? `${(progress.bytesPerSecond / 1024).toFixed(2)} KB/s`
            : "N/A";
          process.stdout.write(` | ${speed}`);
        }
      },
      onComplete: () => {
        console.log("\n‚úÖ S3 processing with progress completed!");
      },
      onError: (error) => {
        console.error("\n‚ùå S3 progress error:", error.message);
      },
    },
  });

  try {
    const testDocument = Buffer.from(
      "# S3 Progress Test\n\nTesting S3 operations with progress tracking."
    );

    console.log("üöÄ Starting S3 processing with progress tracking...");

    // Process with progress tracking
    const result = await client.convertWithTarget(
      [
        {
          kind: "file",
          base64_string: testDocument.toString("base64"),
          filename: "progress-test.md",
        },
      ],
      {
        kind: "s3",
        bucket: process.env.S3_OUTPUT_BUCKET || "protection-plus-test",
        key: `test-outputs/progress-test/result-${Date.now()}.zip`,
        region: process.env.AWS_REGION || "us-east-1",
      },
      {
        to_formats: ["md", "json"],
      }
    );

    console.log(); // New line after progress bar

    if (result.success) {
      console.log("‚úÖ S3 processing with progress tracking successful!");
    } else {
      console.error("‚ùå S3 processing failed:", result.error?.message);
    }
  } catch (error) {
    console.error(
      "\nüí• S3 progress error:",
      error instanceof Error ? error.message : error
    );
  }
}

async function main() {
  console.log("‚òÅÔ∏è Docling SDK - S3 Integration Examples");
  console.log("========================================");

  if (!process.env.DOCLING_URL) {
    console.log("üí° Using default URL: http://localhost:5001");
  }

  if (!process.env.DOCLING_API_KEY) {
    console.log("üîì No API key provided (may be optional)");
  } else {
    console.log("üîê API key configured");
  }

  console.log("\nüìã Environment Variables for S3 Integration:");
  console.log("   Required: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY");
  console.log("   Optional: AWS_REGION, S3_BUCKET, S3_KEY, S3_OUTPUT_BUCKET");
  console.log();

  const hasAwsCredentials = !!(
    process.env.AWS_ACCESS_KEY_ID && process.env.AWS_SECRET_ACCESS_KEY
  );

  if (!hasAwsCredentials) {
    console.log("‚ö†Ô∏è AWS credentials not found - S3 examples will be skipped");
    console.log("   Configure AWS credentials to run S3 integration examples");
    console.log();
  }

  await s3SourceExample();
  await s3TargetExample();
  await s3BatchProcessing();
  await s3WithProgress();

  console.log("\nüéâ S3 integration examples completed!");

  if (hasAwsCredentials) {
    console.log("‚òÅÔ∏è Check your S3 buckets for processed results");
  }
}

if (require.main === module) {
  main().catch(console.error);
}

export {
  s3SourceExample,
  s3TargetExample,
  s3BatchProcessing,
  s3WithProgress,
  main,
};
